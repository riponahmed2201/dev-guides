# Big Data Processing (বিগ ডেটা প্রসেসিং)

সিস্টেম ডিজাইনে যখন বিশাল পরিমাণ ডেটা অ্যানালাইজ বা প্রসেস করতে হয়, তখন সাধারণ ডাটাবেস যথেষ্ঠ নয়। বিগ ডেটা প্রসেসিংয়ে ডেটা ম্যানেজমেন্টের জন্য ব্যবহৃত কোর কনসেপ্টগুলো নিচে আলোচনা করা হলো।

---

## ১. ব্যাচ বনাম স্ট্রিম প্রসেসিং (Batch vs Stream Processing)

| ফিচার          | Batch Processing                             | Stream Processing                             |
| :------------- | :------------------------------------------- | :-------------------------------------------- |
| **পদ্ধতি**     | ডেটা জমিয়ে একবারে প্রসেস করা।                | ডেটা আসার সাথে সাথে রিয়েল-টাইম প্রসেস করা।    |
| **ল্যাটেন্সি** | হাই (ঘন্টা বা দিন লাগতে পারে)।               | লো (মিলি-সেকেন্ড বা সেকেন্ড)।                 |
| **উদা:**       | মাসিক স্যালারি জেনারেশন, আগের দিনের রিপোর্ট। | জিপিএস ট্র্যাকিং, সোশ্যাল মিডিয়া ফিড।         |
| **টুলস:**      | Hadoop MapReduce, Apache Spark.              | Apache Flink, Spark Streaming, Kafka Streams. |

---

## ২. ইটিএল পাইপলাইন (ETL Pipelines)

**ETL** মানে হলো **Extract, Transform, Load**। বিশাল ডেটাসেটকে এক জায়গা থেকে শুরু করে কনভার্ট করে অন্য জায়গায় রাখার পদ্ধতি।

- **Extract:** সোর্স (উদা: DB, Logs) থেকে ডেটা সংগ্রহ করা।
- **Transform:** ডেটাকে ক্লিন করা, ফরম্যাট চেঞ্জ করা এবং বিজনেস লজিক অ্যাপ্লাই করা।
- **Load:** প্রসেস করা ডেটাকে ডেটা ওয়্যারহাউস বা ডেটা লেকে স্টোর করা।

---

## ৩. ম্যাপ-রিডিউস (MapReduce)

এটি একটি প্রোগ্রামিং মডেল যা বিশাল ডেটাকে ডিস্ট্রিবিউটেড ক্লাস্টারে প্রসেস করতে সাহায্য করে।

- **Map Phase:** ইনপুট ডেটাকে ছোট ছোট টুকুরা করে কী-ভ্যালু (Key-Value) পেয়ারে ভাগ করা।
- **Reduce Phase:** একই কি-এর সব ডেটাকে একত্রিত করে ফাইনাল রেজাল্ট বের করা।
- **উপমা:** একটি বড় লাইব্রেরির সব বই গণনার জন্য আপনি ১০ জন বন্ধুকে ১টি করে আলমারি দিলেন (Map), পরে সবাই মিলে মোট সংখ্যা যোগ করলেন (Reduce)।

---

## ৪. ডেটা লেক বনাম ডেটা ওয়্যারহাউস (Data Lake vs Warehouse)

- **Data Lake:** এখানে ডেটা একদম র ফরম্যাটে (Raw format) স্টোর করা হয়। এটি অত্যন্ত সস্তা এবং ইমেজ, ভিডিও, লগ—সব রাখা যায়। (উদা: AWS S3)।
- **Data Warehouse:** এখানে ডেটা স্টোর করার আগে তাকে একটি নির্দিষ্ট স্ট্রাকচারে (Schema) আনা হয়। এটি মূলত বিজনেস অ্যানালিটিক্স এবং কোয়েরি রান করার জন্য ব্যবহৃত হয়। (উদা: Google BigQuery, Snowflake)।

---

## ৫. সাধারণ ইন্টারভিউ প্রশ্নোত্তর (General Q&A)

**প্রশ্ন ১: কখন আপনি Hadoop এর বদলে Spark ব্যবহার করবেন?**
**উত্তর:** যখন আপনার অনেক দ্রুত এবং ইন-মেমরি প্রসেসিং প্রয়োজন হবে, তখন Spark সেরা। Hadoop মূলত ডিস্ক-বেসড প্রসেসিং করে যা তুলনামূলক স্লো কিন্তু সস্তা।

**প্রশ্ন ২: ডেটা স্কিউ (Data Skew) কী?**
**উত্তর:** যখন ডিস্ট্রিবিউটেড সিস্টেমে কোনো একটি নির্দিষ্ট নোডে অন্যদের চেয়ে অনেক বেশি ডেটা চলে আসে, তখন তাকে ডেটা স্কিউ বলে। এটি প্রসেসিং স্লো করে দেয়।

**প্রশ্ন ৩: 'Lambda Architecture' বলতে কী বোঝেন?**
**উত্তর:** এটি এমন একটি আর্কিটেকচার যা একই সাথে ব্যাচ এবং স্ট্রিম প্রসেসিং হ্যান্ডেল করে। এতে ব্যাচ লেয়ার নির্ভুল ডেটা দেয় এবং স্পিড লেয়ার রিয়েল-টাইম ডেটা দেয়।

---

## ৬. সিনারিও ভিত্তিক প্রশ্ন (Scenario-based Questions)

### সিনারিও ১: "আপনি একটি ফ্রড ডিটেকশন সিস্টেম বানাচ্ছেন যা ক্রেডিট কার্ডের সন্দেহজনক লেনদেন ধরবে। আপনি কোন প্রসেসিং বেছে নেবেন?"

**সমাধান:** এখানে অবশ্যই **Stream Processing** (উদা: Apache Flink) বেছে নেব। কারণ ফ্রড ডিটেকশন যদি লেনেদেনের সাথে সাথে না হয়, তবে ইউজারের কার্ড থেকে টাকা চুরি হয়ে যেতে পারে। ১ ঘণ্টা পরের ব্যাচ প্রসেসিং এখানে কোনো কাজে আসবে না।

### সিনারিও ২: "আপনার কাছে ১০ বছরের পুরনো ইউজার লগ আছে যা খুব কম ব্যবহৃত হয়। আপনি এটি কোথায় স্টোর করবেন?"

**সমাধান:** এটি আমি একটি **Data Lake (Cold Storage)** এ রাখব। কারণ এখানে বিশাল পরিমাণ আনস্ট্রাকচারড ডেটা অনেক সস্তায় স্টোর করে রাখা যায় এবং ভবিষ্যতে যদি কখনো গবেষণার প্রয়োজন হয় তখন তা প্রসেস করা যাবে।

---

> [!TIP]
> বিগ ডেটা নিয়ে চিন্তা করার সময় সবসময় ডেটার **Volume (পরিমাণ)**, **Velocity (গতি)** এবং **Variety (বৈচিত্র্য)**—এই ৩টি 'V' মাথায় রাখবেন।
